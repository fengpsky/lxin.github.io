<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Xin Long</title>
 <link href="http://lxin.org/atom.xml" rel="self"/>
 <link href="http://lxin.org"/>
 <updated>2014-04-08T13:39:49+08:00</updated>
 <id>http://lxin.org</id>
 <author>
   <name>Xin Long</name>
   <email>lxin@gmail.com</email>
 </author>

 
 <entry>
   <title>linux vxlan的使用</title>
   <link href="http://lxin.org/linux%20kernel/2014/04/07/linux-vxlan-configure"/>
   <updated>2014-04-07T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20kernel/2014/04/07/linux-vxlan-configure</id>
   <content type="html">
&lt;h4 id=&quot;vxlan&quot;&gt;1.vxlan独立使用时：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;host1:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  ip link add vxlan10 type vxlan id 10 group 239.0.0.10 ttl 4 dev eth1
  ifconfig eth1 192.168.0.10/24 up
  ifconfig vxlan10 192.168.10.10/24 up
  iptables -F
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;host2:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  ip link add vxlan10 type vxlan id 10 group 239.0.0.10 ttl 4 dev eth1
  ifconfig eth1 192.168.0.20/24 up
  ifconfig vxlan10 192.168.10.20/24 up
  iptables -F
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;vxlanovs&quot;&gt;2.vxlan在ovs中的使用:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;host1:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  ovs-vsctl add-br br1
  ifconfig br1 192.168.0.10 netmask 255.255.255.0
  ovs-vsctl add-port br1 vx1 -- set interface vx1 type=vxlan options:remote_ip=192.168.1.11
  ifconfig eth1 192.168.1.10/24 up
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;host2:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  ovs-vsctl add-br br1
  ifconfig br1 192.168.0.11 netmask 255.255.255.0
  ovs-vsctl add-port br1 vx1 -- set interface vx1 type=vxlan options:remote_ip=192.168.1.10
  ifconfig eth1 192.168.1.11/24 up
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;vxlanbridgeovs&quot;&gt;3.vxlan作为设备加入bridge/ovs中:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;host1:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  brctl addbr br2
  ifconfig br2 192.168.3.10/24 up
  ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth1
  brctl addif br2 vxlan0
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;host2:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  brctl addbr br2
  ifconfig br2 192.168.3.11/24 up
  ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth1
  brctl addif br2 vxlan0
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>ovs datapath模块的实现</title>
   <link href="http://lxin.org/linux%20kernel/2014/04/06/ovs-datapah-implementation"/>
   <updated>2014-04-06T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20kernel/2014/04/06/ovs-datapah-implementation</id>
   <content type="html">
&lt;blockquote&gt;
  &lt;p&gt;datapath是openvswitch在内核中的一个模块，是openvswitch的核心，openvswitch还有两用户态的模块，就是ovs-vswitchd和ovsdb-server,但是datapath似乎是很独立的，只要通过netlink给它配置，就能很好地工作.这个模块网上介绍的不是很多，　也不详细，就很奇怪，今天看完后才明白，代码写得太漂亮了，写文字给别人介绍可能还显得多余，　这里，我也只是作个笔记。　另外的两个用户态模块，　有空了也会分析,完整地理解下openflow在kernel中的实现.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;section&quot;&gt;几个关键的结构体&lt;/h3&gt;

&lt;h4 id=&quot;struct-datapath&quot;&gt;struct datapath&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct datapath {
        struct rcu_head rcu;
        struct list_head list_node; //把自己和其它的datapath实例也连接起来

        /* Flow table. */
        struct flow_table table; //action前要查的流表

        /* Switch ports. */
        struct hlist_head *ports; //存放vport的hash

        /* Stats. */
        struct dp_stats_percpu __percpu *stats_percpu;

#ifdef CONFIG_NET_NS
        /* Network namespace ref. */
        struct net *net;
#endif

        u32 user_features;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相当于bridge.ko中的net_bridge, 不同要清晰得多.&lt;/p&gt;

&lt;h4 id=&quot;struct-vport&quot;&gt;struct vport&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;/**
 * struct vport - one port within a datapath
 * @rcu: RCU callback head for deferred destruction.
 * @dp: Datapath to which this port belongs.
 * @upcall_portid: The Netlink port to use for packets received on this port that
 * miss the flow table.
 * @port_no: Index into @dp&#39;s @ports array.
 * @hash_node: Element in @dev_table hash table in vport.c.
 * @dp_hash_node: Element in @datapath-&amp;gt;ports hash table in datapath.c.
 * @ops: Class structure.
 * @percpu_stats: Points to per-CPU statistics used and maintained by vport
 * @stats_lock: Protects @err_stats;
 * @err_stats: Points to error statistics used and maintained by vport
 */
struct vport {
        struct rcu_head rcu;
        struct datapath *dp;
        u32 upcall_portid;
        u16 port_no;

        struct hlist_node hash_node;
        struct hlist_node dp_hash_node;
        const struct vport_ops *ops;

        struct pcpu_sw_netstats __percpu *percpu_stats;

        spinlock_t stats_lock;
        struct vport_err_stats err_stats;
};

struct vport_parms {
        const char *name;
        enum ovs_vport_type type;
        struct nlattr *options;

        /* For ovs_vport_alloc(). */
        struct datapath *dp;
        u16 port_no;
        u32 upcall_portid;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相当于bridge.ko中的net_bridge_port, 代表一个虚拟端口， 注释上面也有了&lt;/p&gt;

&lt;h4 id=&quot;struct-vportops&quot;&gt;struct vport_ops&lt;/h4&gt;

&lt;p&gt;这个端口操作， 对于不同的类型的端口就是不同的实例， 由一个全局一变量来管理:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static const struct vport_ops *vport_ops_list[] = {
        &amp;amp;ovs_netdev_vport_ops,
        &amp;amp;ovs_internal_vport_ops,

#ifdef CONFIG_OPENVSWITCH_GRE
        &amp;amp;ovs_gre_vport_ops,
#endif
#ifdef CONFIG_OPENVSWITCH_VXLAN
        &amp;amp;ovs_vxlan_vport_ops,
#endif
};
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;struct-ovsnet&quot;&gt;struct ovs_net&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct ovs_net {
        struct list_head dps;
        struct work_struct dp_notify_work;
        struct vport_net vport_net;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不用解释，看看每个ns里都有什么全局信息&lt;/p&gt;

&lt;h4 id=&quot;flowtable&quot;&gt;flow_table&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct table_instance {
        struct flex_array *buckets;
        unsigned int n_buckets;
        struct rcu_head rcu;
        int node_ver;
        u32 hash_seed;
        bool keep_flows;
};

struct flow_table {
        struct table_instance __rcu *ti;
        struct list_head mask_list;
        unsigned long last_rehash;
        unsigned int count;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个datapath对应一个的流表，也就相当于bridge.ko中的net_bridge_fdb_entry.&lt;/p&gt;

&lt;h4 id=&quot;struct-swflow&quot;&gt;struct sw_flow&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct sw_flow {
        struct rcu_head rcu;
        struct hlist_node hash_node[2];
        u32 hash;

        struct sw_flow_key key;
        struct sw_flow_key unmasked_key;
        struct sw_flow_mask *mask;
        struct sw_flow_actions __rcu *sf_acts;
        struct sw_flow_stats stats;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转发时用来保存flow信息在skb cb当中&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;初始化&lt;/h3&gt;

&lt;h4 id=&quot;section-2&quot;&gt;注册模块&lt;/h4&gt;

&lt;p&gt;dp_init()中&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;ovs_flow_init()流表初始化, flow_cache, 全局cache流表的创建&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ovs_vport_init()端口初始化, dev_table, 全局hash表的初始化&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;register_pernet_device(&amp;amp;ovs_net_ops)每名字空间操作注册, ovs_init_net()主要完成对每名字空间变量里dps和dp_notify_work的初始化&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通知链注册,内核框架&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;dp_register_genl()：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; for (i = 0; i &amp;lt; ARRAY_SIZE(dp_genl_families); i++) {
         const struct genl_family_and_ops *f = &amp;amp;dp_genl_families[i];

         f-&amp;gt;family-&amp;gt;ops = f-&amp;gt;ops;
         f-&amp;gt;family-&amp;gt;n_ops = f-&amp;gt;n_ops;
         f-&amp;gt;family-&amp;gt;mcgrps = f-&amp;gt;group;
         f-&amp;gt;family-&amp;gt;n_mcgrps = f-&amp;gt;group ? 1 : 0;
         err = genl_register_family(f-&amp;gt;family);
         if (err)
                 goto error;
         n_registered++;
 }
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;这个函数重要，通过genetic框架注册了datapath, vport, flow, packet四类与用户空间来交互操作的接口:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; static const struct genl_family_and_ops dp_genl_families[] = {
         { &amp;amp;dp_datapath_genl_family,
           dp_datapath_genl_ops, ARRAY_SIZE(dp_datapath_genl_ops),
           &amp;amp;ovs_dp_datapath_multicast_group },
         { &amp;amp;dp_vport_genl_family,
           dp_vport_genl_ops, ARRAY_SIZE(dp_vport_genl_ops),
           &amp;amp;ovs_dp_vport_multicast_group },
         { &amp;amp;dp_flow_genl_family,
           dp_flow_genl_ops, ARRAY_SIZE(dp_flow_genl_ops),
           &amp;amp;ovs_dp_flow_multicast_group },
         { &amp;amp;dp_packet_genl_family,
           dp_packet_genl_ops, ARRAY_SIZE(dp_packet_genl_ops),
           NULL },
 };
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;具体的见其它genl框架的分析&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-3&quot;&gt;创建&lt;/h3&gt;

&lt;h4 id=&quot;datapath&quot;&gt;datapath&lt;/h4&gt;

&lt;p&gt;ovs_dp_cmd_new():&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    static const struct genl_ops dp_datapath_genl_ops[] = {
            { .cmd = OVS_DP_CMD_NEW,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = datapath_policy,
              .doit = ovs_dp_cmd_new
            },
            { .cmd = OVS_DP_CMD_DEL,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = datapath_policy,
              .doit = ovs_dp_cmd_del
            },
            { .cmd = OVS_DP_CMD_GET,
              .flags = 0,               /* OK for unprivileged users. */
              .policy = datapath_policy,
              .doit = ovs_dp_cmd_get,
              .dumpit = ovs_dp_cmd_dump
            },
            { .cmd = OVS_DP_CMD_SET,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = datapath_policy,
              .doit = ovs_dp_cmd_set,
            },
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;申请dp,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在ovs_flow_tbl_init(&amp;amp;dp-&amp;gt;table)中申请flow_table&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;申请dp-&amp;gt;ports hash结构&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;并调用vport = new_vport(&amp;amp;parms);创建一个internal类型的vport, params为struct vport_parms parms类型，用来描述一个vport.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;构建一个notify信息， 发出去。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;vport&quot;&gt;vport&lt;/h3&gt;

&lt;p&gt;ovs_vport_cmd_new():&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    static const struct genl_ops dp_vport_genl_ops[] = {
            { .cmd = OVS_VPORT_CMD_NEW,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = vport_policy,
              .doit = ovs_vport_cmd_new
            },
            { .cmd = OVS_VPORT_CMD_DEL,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = vport_policy,
              .doit = ovs_vport_cmd_del
            },
            { .cmd = OVS_VPORT_CMD_GET,
              .flags = 0,               /* OK for unprivileged users. */
              .policy = vport_policy,
              .doit = ovs_vport_cmd_get,
              .dumpit = ovs_vport_cmd_dump
            },
            { .cmd = OVS_VPORT_CMD_SET,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = vport_policy,
              .doit = ovs_vport_cmd_set,
            },
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;通过dp_ifindex找到dp,然后利用struct params组织vport信息。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通过new_vport创建并添加进dp&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;调用ovs_vport_add创建, 在这个函数中， 会调用相应的create函数，如vxlan就是vxlan_tnl_create:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; const struct vport_ops ovs_vxlan_vport_ops = {
         .type           = OVS_VPORT_TYPE_VXLAN,
         .create         = vxlan_tnl_create,
         .destroy        = vxlan_tnl_destroy,
         .get_name       = vxlan_get_name,
         .get_options    = vxlan_get_options,
         .send           = vxlan_tnl_send,
 };
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;在vxlan_tnl_create中会调用先申请一个vport和vxlan_port， 这两个的关系vxlan_port=net_priv(vport).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;然后再申请vs, 调用我们前面分析过的vxlan_sock_add(), 再与vxlan_port关联, vxlan_port-&amp;gt;vs = vs;&lt;/p&gt;

    &lt;p&gt;可以看出来的是， 这个过程中间，对于vxlan设备来讲没有产生任何net_device，因些ifconfig -a 也就不会看到，有的也只是一个vport, 尽管中间抽取了一些vxlan源接口&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;最后就是加入到dp中， 并发送notify信息&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;flowtable-1&quot;&gt;flow_table&lt;/h4&gt;

&lt;p&gt;ovs_flow_cmd_new_or_set():&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    static const struct genl_ops dp_flow_genl_ops[] = {
            { .cmd = OVS_FLOW_CMD_NEW,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = flow_policy,
              .doit = ovs_flow_cmd_new_or_set
            },
            { .cmd = OVS_FLOW_CMD_DEL,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = flow_policy,
              .doit = ovs_flow_cmd_del
            },
            { .cmd = OVS_FLOW_CMD_GET,
              .flags = 0,               /* OK for unprivileged users. */
              .policy = flow_policy,
              .doit = ovs_flow_cmd_get,
              .dumpit = ovs_flow_cmd_dump
            },
            { .cmd = OVS_FLOW_CMD_SET,
              .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
              .policy = flow_policy,
              .doit = ovs_flow_cmd_new_or_set,
            },
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个由于与vxlan关系不大，以后再写&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;接收&lt;/h3&gt;

&lt;h4 id=&quot;netdev&quot;&gt;net_dev&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;从栈议栈中的netif_receive_skb开始吧，这个函数中会调用dev-&amp;gt;rx_hander, 如果一个设备被加进了datapath, 那么为!NULL, 当作一个net_dev的设备添加进datapath, 就为netdev_frame_hook&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;接着获取vport = ovs_netdev_get_vport(skb-&amp;gt;dev); 调用netdev_port_receive(), 再调用ovs_vport_receive(),&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;接着再调用ovs_dp_process_received_packet(), 这个函数才是真正接收到后进行处理的函数&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; error = ovs_flow_extract(skb, p-&amp;gt;port_no, &amp;amp;key);
 if (unlikely(error)) {
         kfree_skb(skb);
         return;
 }

 /* Look up flow. */
 flow = ovs_flow_tbl_lookup_stats(&amp;amp;dp-&amp;gt;table, &amp;amp;key, &amp;amp;n_mask_hit);
 if (unlikely(!flow)) {
         struct dp_upcall_info upcall;

         upcall.cmd = OVS_PACKET_CMD_MISS;
         upcall.key = &amp;amp;key;
         upcall.userdata = NULL;
         upcall.portid = p-&amp;gt;upcall_portid;
         ovs_dp_upcall(dp, skb, &amp;amp;upcall);
         consume_skb(skb);
         stats_counter = &amp;amp;stats-&amp;gt;n_missed;
         goto out;
 }

 OVS_CB(skb)-&amp;gt;flow = flow;
 OVS_CB(skb)-&amp;gt;pkt_key = &amp;amp;key;

 ovs_flow_stats_update(OVS_CB(skb)-&amp;gt;flow, skb);
 ovs_execute_actions(dp, skb);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;拿到流信息，更新进skb中的cb上，再在ovs_execute_actions中执行, 其中ovs_flow_extract是一个重要函数，在它里面对skb进行了解包分析&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ovs_execute_actions中， 拿到struct sw_flow_actions *acts = rcu_dereference(OVS_CB(skb)-&amp;gt;flow-&amp;gt;sf_acts);再调用do_execute_actions(), 在这个函数中，进行转发：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; for (a = attr, rem = len; rem &amp;gt; 0;
      a = nla_next(a, &amp;amp;rem)) {
         int err = 0;

         if (prev_port != -1) {
                 do_output(dp, skb_clone(skb, GFP_ATOMIC), prev_port);
                 prev_port = -1;
         }

         switch (nla_type(a)) {
         case OVS_ACTION_ATTR_OUTPUT:
                 prev_port = nla_get_u32(a);
                 break;
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;do_output是最终的调用,进入到这个函数中你就看到， 通过port_no查端口，而port_no应该就是在ovs_flow_tbl_lookup_stats()中拿到，还没来得及看， 并且ovs_vport_send发送了。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;vxlan&quot;&gt;vxlan&lt;/h4&gt;

&lt;p&gt;你会经常看到在ovs中， 有vport，但没有对应的net_device, 像添加vxlan类型设备时，它的接收自然也就不会靠rx_handler了， 对于vxlan, 很显然，有一个监听着的sock,过来的包自然找得到， 通过以前的分析，知道vxlan sock的从udp来看接收包函数为vxlan_udp_encap_recv, 最终为vs-&amp;gt;rcv, 即vxlan_rcv,当然名字相同，但声明了static, 在ovs中另有定义，
1. 在vxlan_rcv中， 会调用ovs_vport_receive(),回到上述的第三步&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;发送&lt;/h3&gt;

&lt;p&gt;其实就是转发， 接do_output后的ovs_vport_send()&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;vport-&amp;gt;ops-&amp;gt;send(vport, skb); 又回到了vport_ops, 接口自己实现， vxlan当初注册的是vxlan_tnl_send()&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;打开这个函数后，你会发现它酷似当年在vxlan.ko中看到的vxlan_rcv,本来嘛，就是接口的抽取，就不要感到奇怪了,不过它直接调用的是vxlan_xmit_skb,所以得做vxlan_xmit_one的工作.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;另外不知道有没有发现， 这种模式下vxlan没有fdb的学习接口， 想想为什么？ 想想配置时两者的区别就有了答案&lt;/p&gt;

&lt;p&gt;大概看了下， 其它gre与vxlan在vxlan的实现很相似， 所以包括配置也很想像，可谓异曲同工.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>vxlan的内核实现</title>
   <link href="http://lxin.org/linux%20kernel/2014/04/05/vxlan-implementation"/>
   <updated>2014-04-05T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20kernel/2014/04/05/vxlan-implementation</id>
   <content type="html">
&lt;h3 id=&quot;section&quot;&gt;几个关键的结构体&lt;/h3&gt;

&lt;h4 id=&quot;vxlannet&quot;&gt;vxlan_net&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct vxlan_net {
        struct list_head  vxlan_list;
        struct hlist_head sock_list[PORT_HASH_SIZE];
        spinlock_t        sock_lock;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个每名字空间的数据块。&lt;/p&gt;

&lt;h4 id=&quot;struct-vxlansock&quot;&gt;struct vxlan_sock&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct vxlan_sock {
        struct hlist_node hlist;
        vxlan_rcv_t      *rcv;//接收函数，但事实上入口接收是encap_rcv()
        void             *data;
        struct work_struct del_work;
        struct socket    *sock;
        struct rcu_head   rcu;
        struct hlist_head vni_list[VNI_HASH_SIZE];//关联vni
        atomic_t          refcnt;
        struct udp_offload udp_offloads;//vxlan udp offloads使用
};
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;struct-vxlandev&quot;&gt;struct vxlan_dev&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct vxlan_dev {
        struct hlist_node hlist;        /* vni hash table */
        struct list_head  next;         /* vxlan&#39;s per namespace list */
        struct vxlan_sock *vn_sock;     /* listening socket */
        struct net_device *dev;
        struct vxlan_rdst default_dst;  /* default destination */
        union vxlan_addr  saddr;        /* source address */
        __be16            dst_port;
        __u16             port_min;     /* source port range */
        __u16             port_max;
        __u8              tos;          /* TOS override */
        __u8              ttl;
        u32               flags;        /* VXLAN_F_* below */

        struct work_struct sock_work;
        struct work_struct igmp_join;
        struct work_struct igmp_leave;//三个工作队列

        unsigned long     age_interval;
        struct timer_list age_timer;
        spinlock_t        hash_lock;
        unsigned int      addrcnt;
        unsigned int      addrmax;

        struct hlist_head fdb_head[FDB_HASH_SIZE];//转发表
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;net_device的private成员，即vxlan设备的私有信息&lt;/p&gt;

&lt;h4 id=&quot;struct-vxlanrdst&quot;&gt;struct vxlan_rdst&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct vxlan_rdst {
        union vxlan_addr         remote_ip;
        __be16                   remote_port;
        u32                      remote_vni;
        u32                      remote_ifindex;
        struct list_head         list;
        struct rcu_head          rcu;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;存在在vxlan_fdb的remotes中用于描述远端的地址信息.&lt;/p&gt;

&lt;h4 id=&quot;struct-vxlanfdb&quot;&gt;struct vxlan_fdb&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;/* Forwarding table entry */
struct vxlan_fdb {
        struct hlist_node hlist;        /* linked list of entries */
        struct rcu_head   rcu;
        unsigned long     updated;      /* jiffies */
        unsigned long     used;
        struct list_head  remotes;//struct vxlan_rdst链表
        u16               state;        /* see ndm_state */
        u8                flags;        /* see ndm_flags */
        u8                eth_addr[ETH_ALEN];//
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转发表，存在在vxlan_dev的fdb_head中&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;初始化&lt;/h3&gt;

&lt;h4 id=&quot;section-2&quot;&gt;注册模块&lt;/h4&gt;

&lt;h5 id=&quot;vxlaninitmoduledriversnetvxlanc-&quot;&gt;vxlan_init_module(),drivers/net/vxlan.c, 也是比较清晰的几行代码.&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;vxlan_wq = alloc_workqueue(“vxlan”, 0, 0);申请一个工作队列，后面会说到它的作用. vxlan_wq是global的&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;rc = register_pernet_subsys(&amp;amp;vxlan_net_ops);注册一个每名字空间的操作&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; static struct pernet_operations vxlan_net_ops = {
         .init = vxlan_init_net,
         .id   = &amp;amp;vxlan_net_id,
         .size = sizeof(struct vxlan_net),
 };
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;rc = register_netdevice_notifier(&amp;amp;vxlan_notifier_block); 网络通知链&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;rc = rtnl_link_register(&amp;amp;vxlan_link_ops);rtnl框架注册link操作,见rtnl框架分析&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;其中，在vxlan_init_net()中，获取一块空间，　用来存放一个pernet的vxlan_net,　并进行初始化。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;section-3&quot;&gt;创建&lt;/h4&gt;

&lt;h5 id=&quot;vxlannewlink&quot;&gt;vxlan_newlink()中完成&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;这是一个rtnl框架注册过的函数，　因此rtnl执行到这的时候net_device己经创建，并且己调用相应的setup()函数设置过,见rtnl_netlink分析.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;vxlan_setup()中，为net_device设置了ops, 以及初始化fdb, 还有三个工作队列&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    static const struct net_device_ops vxlan_netdev_ops = {
            .ndo_init               = vxlan_init,
            .ndo_uninit             = vxlan_uninit,
            .ndo_open               = vxlan_open,
            .ndo_stop               = vxlan_stop,
            .ndo_start_xmit         = vxlan_xmit,
            .ndo_get_stats64        = ip_tunnel_get_stats64,
            .ndo_set_rx_mode        = vxlan_set_multicast_list,
            .ndo_change_mtu         = vxlan_change_mtu,
            .ndo_validate_addr      = eth_validate_addr,
            .ndo_set_mac_address    = eth_mac_addr,
            .ndo_fdb_add            = vxlan_fdb_add,
            .ndo_fdb_del            = vxlan_fdb_delete,
            .ndo_fdb_dump           = vxlan_fdb_dump,
    };

    INIT_WORK(&amp;amp;vxlan-&amp;gt;igmp_join, vxlan_igmp_join);
    INIT_WORK(&amp;amp;vxlan-&amp;gt;igmp_leave, vxlan_igmp_leave);
    INIT_WORK(&amp;amp;vxlan-&amp;gt;sock_work, vxlan_sock_work);

最后面的这个工作队列会在netlink的时候触发(ndo_init).在它里面会创建传说中的vxlan_sock
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;接着将其private还原成struct vxlan_dev,进行初始化, 如：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; struct vxlan_rdst *dst = &amp;amp;vxlan-&amp;gt;default_dst;
 dst-&amp;gt;remote_vni = vni;
 dst-&amp;gt;remote_ip.sin.sin_addr.s_addr = nla_get_be32(data[IFLA_VXLAN_GROUP]);
 dst-&amp;gt;remote_ip.sa.sa_family = AF_INET;
 vxlan-&amp;gt;saddr.sin.sin_addr.s_addr = nla_get_be32(data[IFLA_VXLAN_LOCAL]);
 vxlan-&amp;gt;saddr.sa.sa_family = AF_INET;
 dst-&amp;gt;remote_ifindex = nla_get_u32(data[IFLA_VXLAN_LINK]
 ....
 vxlan-&amp;gt;dst_port = nla_get_be16(data[IFLA_VXLAN_PORT]);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;并且会通过vxlan_find_vni(net, vni, vxlan-&amp;gt;dst_port)对vni进行duplicate检测&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;调用vxlan_fdb_create()创建条转发项&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; err = vxlan_fdb_create(vxlan, all_zeros_mac,
                        &amp;amp;vxlan-&amp;gt;default_dst.remote_ip,
                        NUD_REACHABLE|NUD_PERMANENT,
                        NLM_F_EXCL|NLM_F_CREATE,
                        vxlan-&amp;gt;dst_port,
                        vxlan-&amp;gt;default_dst.remote_vni,
                        vxlan-&amp;gt;default_dst.remote_ifindex,
                        NTF_SELF);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;5.注册net_device,同时在register_netdevice时利用ndo_init  queue_work(vxlan_wq, &amp;amp;vxlan-&amp;gt;sock_work);触发vxlan_sock_work(). 此时就能用ifconfig 看到设备了,并再添加进行vxlan_net中。&lt;/p&gt;

&lt;h4 id=&quot;vxlansocket&quot;&gt;创建vxlan_socket&lt;/h4&gt;

&lt;h5 id=&quot;vxlansockwork&quot;&gt;在vxlan_sock_work()中完成&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;vxlan_sock_add(),调用vxlan_socket_create(),如果失败，就用vxlan_find_sock()在pernet中去根据port去找&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在,vxlan_socket_create中,先申请一个vs. 再创建一个sock&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; if (ipv6)
         sock = create_v6_sock(net, port);
 else
         sock = create_v4_sock(net, port);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;相信你会看个这个很水的sock,连创建函数都是定制的, create_v4_sock:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; struct sockaddr_in vxlan_addr = {
         .sin_family = AF_INET,
         .sin_addr.s_addr = htonl(INADDR_ANY),
         .sin_port = port,
 };
 rc = sock_create_kern(AF_INET, SOCK_DGRAM, IPPROTO_UDP, &amp;amp;sock);
 ....
 rc = kernel_bind(sock, (struct sockaddr *) &amp;amp;vxlan_addr,
                  sizeof(vxlan_addr));
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;上面才回到通用创建的接口上。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;还有一个重要的操作，就是vs-&amp;gt;rcv = rcv，设置接收函数.当然先调用函数调用在下面设置：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; udp_sk(sk)-&amp;gt;encap_type = 1;
 udp_sk(sk)-&amp;gt;encap_rcv = vxlan_udp_encap_recv;
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;对于这个函数，会在接收时重点说。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-4&quot;&gt;打开&lt;/h3&gt;

&lt;h5 id=&quot;ifconfig-vxlan-up-vxlanopen&quot;&gt;既然是虚拟网络设置，就按个这个框架来说明，ifconfig vxlan* up, 调用vxlan_open()，做了两件事情&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;queue_work(vxlan_wq, &amp;amp;vxlan-&amp;gt;igmp_join);唤醒另一个工作队列&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;还有一个定时器，后面再做深入&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;vxlan_igmp_join()中，主要还是调用通用接口来设置vxlan_sock加入mgroup.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; struct sock *sk = vs-&amp;gt;sock-&amp;gt;sk;
 int ifindex = vxlan-&amp;gt;default_dst.remote_ifindex;
 struct ip_mreqn mreq = {
                     .imr_multiaddr.s_addr   = ip-&amp;gt;sin.sin_addr.s_addr,
                     .imr_ifindex            = ifindex,
             };

             ip_mc_join_group(sk, &amp;amp;mreq);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;很显然，从一开始到现在都是让这个sock能有接收包的能力，似乎在发包时没有让你承载travel ip栈的想法&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;发送&lt;/h3&gt;

&lt;h5 id=&quot;ndostartxmitvxlanxmit-ether&quot;&gt;ndo_start_xmit函数是vxlan_xmit, 自然由它完成发送,此时的发是一个完整的ether包。&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;f = vxlan_find_mac(vxlan, eth-&amp;gt;h_dest); 先去fdb表中找vxlan_fdb转发项,相当于协议栈中的dst_entry, 当然是通过mac地址来找的&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果没有找着，一开始还没学习通常都会这样，就会执行f = vxlan_find_mac(vxlan, all_zeros_mac); all_zeros_mac对应的那个项就是我们一开始添加的那个，所以应该会有期望结果返回，而返回的结中的目的地址就是添加时的参数里的那个多播地址，　就用在这里了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当然找着的转发项可能存在多个目的地址，因此：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; list_for_each_entry_rcu(rdst, &amp;amp;f-&amp;gt;remotes, list) {
         struct sk_buff *skb1;

         if (!fdst) {
                 fdst = rdst;
                 continue;
         }
         skb1 = skb_clone(skb, GFP_ATOMIC);
         if (skb1)
                 vxlan_xmit_one(skb1, dev, rdst, did_rsc);
 }

 if (fdst)
         vxlan_xmit_one(skb, dev, fdst, did_rsc);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;可见在vxlan_xmit 中完成了转发项的查找，再调用vxlan_xmit_one, 关键性的几行代码:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; fl4.flowi4_oif = rdst-&amp;gt;remote_ifindex;
 fl4.flowi4_tos = RT_TOS(tos);
 fl4.daddr = dst-&amp;gt;sin.sin_addr.s_addr;
 fl4.saddr = vxlan-&amp;gt;saddr.sin.sin_addr.s_addr;

 rt = ip_route_output_key(dev_net(dev), &amp;amp;fl4);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;这便是构建flowi,完成路由的查找，　再往后的发送就是vxlan_xmit_skb了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在这个函数中,非常重要的一步:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; min_headroom = LL_RESERVED_SPACE(rt-&amp;gt;dst.dev) + rt-&amp;gt;dst.header_len
                 + VXLAN_HLEN + sizeof(struct iphdr)
                 + (vlan_tx_tag_present(skb) ? VLAN_HLEN : 0);

 /* Need space for new headers (invalidates iph ptr) */
 err = skb_cow_head(skb, min_headroom);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;skb_cow_head是用来拷贝和扩展头部空间的,这个函数有点麻烦，就不分析了.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; vxh = (struct vxlanhdr *) __skb_push(skb, sizeof(vxh));
 vxh-&amp;gt;vx_flags = htonl(VXLAN_FLAGS);
 vxh-&amp;gt;vx_vni = vni;

 __skb_push(skb, sizeof(uh));
 skb_reset_transport_header(skb);
 uh = udp_hdr(skb);

 uh-&amp;gt;dest = dst_port;
 uh-&amp;gt;source = src_port;

 uh-&amp;gt;len = htons(skb-&amp;gt;len);
 uh-&amp;gt;check = 0;

 err = handle_offloads(skb);
 if (err)
         return err;

 skb_set_owner_w(skb, vs-&amp;gt;sock-&amp;gt;sk);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;这些代码不用说了，　就是扩展完之后，进行vxlan头部，　和udp头部的填充的,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;再接iptunnel_xmit函数完成发送，　这个函数是ip_tunnel添加ip头部并发送的必调函数。见ip_tunnel框架的分析.&lt;/p&gt;

    &lt;p&gt;另外在vxlan整个代码处理过程中，虽说它是个tunnel，　但并没有套用用ip_tunnel这个框架，不明白为什么加载时还要依赖ip_tunnel这个模块&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-6&quot;&gt;接收&lt;/h3&gt;

&lt;h5 id=&quot;vsudpsockencaprcv--vxlanudpencaprecvudprcv&quot;&gt;上面提到了vs对应的udp_sock中的encap_rcv = vxlan_udp_encap_recv，　则在接收于会交给这个接口，　默认你是了解udp_rcv的。&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;此时到的数据是一个ether包+vxlan头部,因此首先获取vxh头，然后再利用ip_tunnel的另一个接口移动头部指针到正确位置:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; vxh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
 ...
 if (iptunnel_pull_header(skb, VXLAN_HLEN, htons(ETH_P_TEB)))
     goto drop;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;接下来就调用到了vxlan_rcv:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; vxlan = vxlan_vs_find_vni(vs, vni);
 if (!vxlan)
         goto drop;
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;先判断vni是否存在，不存在就drop.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; if ((vxlan-&amp;gt;flags &amp;amp; VXLAN_F_LEARN) &amp;amp;&amp;amp;
     vxlan_snoop(skb-&amp;gt;dev, &amp;amp;saddr, eth_hdr(skb)-&amp;gt;h_source))
         goto drop;
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;vxlan_snoop重要的一步，是fdb项的学习,它会把此次通信的包以mac为索引添加进fdb表.当然对于这种表，有添加就得有清理，类似于邻居表或路由表，还记得初始化那会一个定时器吗，没有讲，它的作用就是来根据时间做表的清理工作的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;到此，就剩下把这个完整的ether包上交了, netif_rx(),好经典好古老的一个接口，相信你还记得它。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>rtnetlink框架</title>
   <link href="http://lxin.org/linux%20kernel/2014/04/04/rtnetlink-architecture"/>
   <updated>2014-04-04T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20kernel/2014/04/04/rtnetlink-architecture</id>
   <content type="html">
&lt;h3 id=&quot;section&quot;&gt;几个关键的结构体&lt;/h3&gt;

&lt;h4 id=&quot;strct-rtnllinkops&quot;&gt;strct rtnl_link_ops&lt;/h4&gt;

&lt;p&gt;include/net/rtnetlink.h中&lt;/p&gt;

&lt;p&gt;描述不同net_device的link操作，如new, dump, set等,因些一个设备想通过rtnetfilter配置就需要在这个框架中注册一项,如vxlan:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static struct rtnl_link_ops vxlan_link_ops __read_mostly = {
        .kind           = &quot;vxlan&quot;,
        .maxtype        = IFLA_VXLAN_MAX,
        .policy         = vxlan_policy,
        .priv_size      = sizeof(struct vxlan_dev),
        .setup          = vxlan_setup,
        .validate       = vxlan_validate,
        .newlink        = vxlan_newlink,
        .dellink        = vxlan_dellink,
        .get_size       = vxlan_get_size,
        .fill_info      = vxlan_fill_info,
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在其初始化时,rc = rtnl_link_register(&amp;amp;vxlan_link_ops);算是一种对于不同link的扩展.这个函数会在static LIST_HEAD(link_ops)中添加一项, 当然link_ops也是global的&lt;/p&gt;

&lt;h4 id=&quot;rtnllink&quot;&gt;rtnl_link&lt;/h4&gt;

&lt;p&gt;net/core/rtnetlink.c中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct rtnl_link {
        rtnl_doit_func          doit;
        rtnl_dumpit_func        dumpit;
        rtnl_calcit_func        calcit;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;描述真实操作的函数，构建在rtnl_msg_handlers中，又一个global变量. 可通过rtnl_register()进行注册.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void rtnl_register(int protocol, int msgtype,
                   rtnl_doit_func doit, rtnl_dumpit_func dumpit,
                   rtnl_calcit_func calcit)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有想通过rtnetfilter进行配置的操作可通过它来进行注册.可能会问到它与link_ops的关系。　事实上rtnl_register注册的link操作会调用link_ops这个VT进行操作.似乎刚注册进rtnl框架中的rtnl_link又变成了一个新框架给link_ops用。没办法，内核里面从来都是框架里面套框架 。rtnl自身在初始化时就通过这个接口添加了一堆重要的接口。rtnetlink_init()&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;初始化&lt;/h3&gt;

&lt;p&gt;rtnetlink_init(), net/core/rtnetlink.c&lt;/p&gt;

&lt;p&gt;最清晰的几行代码.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;register_pernet_subsys(&amp;amp;rtnetlink_net_ops)，网络名字空间框架的操作，另一篇中会提到，只要知道在创建名字空间时, init会被调用，删除时exit会被调用.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; static struct pernet_operations rtnetlink_net_ops = {
         .init = rtnetlink_net_init,
         .exit = rtnetlink_net_exit,
 };
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;而在rtnetlink_net_init()中　sk = netlink_kernel_create(net, NETLINK_ROUTE, &amp;amp;cfg);　就被创建，　接netfiler框架分析, 还是需要提下rtnetlink_rcv()会在些被设置rcv函数，因为下面会说到。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;register_netdevice_notifier(&amp;amp;rtnetlink_dev_notifier); 网络通知链接框架的操作，　不用多说。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;rtnl_register(…), 下来是一堆的这样的函数，上面也已经提到过其作用。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-2&quot;&gt;接收&lt;/h3&gt;

&lt;p&gt;rtnetlink_rcv-&amp;gt;netlink_rcv_skb-&amp;gt;rtnetlink_rcv_msg&lt;/p&gt;

&lt;p&gt;在rtnetlink_rcv_msg()中，分两类：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;为dump操作:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; dumpit = rtnl_get_dumpit(family, type);
 if (dumpit == NULL)
         return -EOPNOTSUPP;
 calcit = rtnl_get_calcit(family, type);
 if (calcit)
         min_dump_alloc = calcit(skb, nlh);

 __rtnl_unlock();
 rtnl = net-&amp;gt;rtnl;
 {
         struct netlink_dump_control c = {
                 .dump           = dumpit,
                 .min_dump_alloc = min_dump_alloc,
         };
         err = netlink_dump_start(rtnl, skb, nlh, &amp;amp;c);
 }
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;会先后从rtnl_msg_handlers中取dumpit接口，　和calcit接口，　后者拿到后会直接调用计算需要的空间大小，再组合传入netlink_dump_start()中&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;为do操作:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; doit = rtnl_get_doit(family, type);
 if (doit == NULL)
         return -EOPNOTSUPP;

 return doit(skb, nlh);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;获取后直接执行&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在netlink_dump_start()中&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; sk = netlink_lookup(sock_net(ssk), ssk-&amp;gt;sk_protocol, NETLINK_CB(skb).portid);
 ...
 cb = &amp;amp;nlk-&amp;gt;cb;
 memset(cb, 0, sizeof(cb));
 cb-&amp;gt;dump = control-&amp;gt;dump;
 cb-&amp;gt;done = control-&amp;gt;done;
 cb-&amp;gt;nlh = nlh;
 cb-&amp;gt;data = control-&amp;gt;data;
 cb-&amp;gt;module = control-&amp;gt;module;
 cb-&amp;gt;min_dump_alloc = control-&amp;gt;min_dump_alloc;
 cb-&amp;gt;skb = skb;

 nlk-&amp;gt;cb_running = true;

 mutex_unlock(nlk-&amp;gt;cb_mutex);

 ret = netlink_dump(sk);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;找着出去的socket,构建个cb,传入netlink_dump(), 上面的control就是1中的c&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; skb = netlink_alloc_skb(sk, alloc_size, nlk-&amp;gt;portid, GFP_KERNEL);
 if (!skb)
         goto errout_skb;
 netlink_skb_set_owner_r(skb, sk);

 len = cb-&amp;gt;dump(skb, cb);

 if (len &amp;gt; 0) {
         mutex_unlock(nlk-&amp;gt;cb_mutex);

         if (sk_filter(sk, skb))
                 kfree_skb(skb);
         else
                 __netlink_sendskb(sk, skb);
         return 0;
 }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;netlink_dump()中，　重新生成skb, 并与外面的sock绑定, 再调用cb-&amp;gt;dump(即c中的dumpit),最后调用__netlink_sendskb()，回到netlink的框架了&lt;/p&gt;

&lt;p&gt;先写到这rtnl_newlink()与rtnl_dump_all()改天另作分析，　其中前者中就调用了rtnl_link_ops-&amp;gt;net_link();&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>netlink框架</title>
   <link href="http://lxin.org/linux%20kernel/2014/04/04/netlink-architecture"/>
   <updated>2014-04-04T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20kernel/2014/04/04/netlink-architecture</id>
   <content type="html">
&lt;h3 id=&quot;section&quot;&gt;几个关键的结构体&lt;/h3&gt;

&lt;h4 id=&quot;netlinktable&quot;&gt;netlink_table&lt;/h4&gt;

&lt;p&gt;net/netlink/af_netlink.h中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct netlink_table {
        struct nl_portid_hash   hash;//放这个协议的sock实例
        struct hlist_head       mc_list;//组播的sock实例列表
        struct listeners __rcu  *listeners;
        unsigned int            flags;
        unsigned int            groups;//组的个数
        struct mutex            *cb_mutex;
        struct module           *module;
        void                    (*bind)(int group);//要加入的组
        bool                    (*compare)(struct net *net, struct sock *sock);//查找时的对比参数
        int                     registered;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;描述一个netlink的procotol, netlink.h中定义了目前所有的，包括最常用的NETLINK_ROUTE, NETLINK_GENERIC;　这个结构体包含了最关键的nl_portid_hash类型结构的字段.
struct netlink_table *nl_table 是管理整个netlink procotol的表, 是global的.其创建是在af_netlink.c中的netlink_proto_init()中，各自初始化则是在相应的模块中，如NETLINK_ROUTE是在rtnetlink.c里的rtnetlink_net_init()中。&lt;/p&gt;

&lt;h4 id=&quot;netlinksock&quot;&gt;netlink_sock&lt;/h4&gt;

&lt;p&gt;net/netlink/af_netlink.h中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct netlink_sock {
        /* struct sock has to be the first member of netlink_sock */
        struct sock             sk;
        u32                     portid;
        u32                     dst_portid;
        u32                     dst_group;
        u32                     flags;
        u32                     subscriptions;
        u32                     ngroups;
        unsigned long           *groups;
        unsigned long           state;
        wait_queue_head_t       wait;
        bool                    cb_running;
        struct netlink_callback cb;
        struct mutex            *cb_mutex;
        struct mutex            cb_def_mutex;
        void                    (*netlink_rcv)(struct sk_buff *skb);
        void                    (*netlink_bind)(int group);
        struct module           *module;
#ifdef CONFIG_NETLINK_MMAP
        struct mutex            pg_vec_lock;
        struct netlink_ring     rx_ring;
        struct netlink_ring     tx_ring;
        atomic_t                mapped;
#endif /* CONFIG_NETLINK_MMAP */
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从名称就可以看出来是netlink自己的传输控制块， 不失一般性， 它封装的依然是我们熟悉的struct sock.里面再包含了一些特有的信息， 重要的如portid, netlink_rcv().&lt;/p&gt;

&lt;h4 id=&quot;nlmsghdr--sockaddrnl&quot;&gt;nlmsghdr &amp;amp; sockaddr_nl&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;struct sockaddr_nl {
        __kernel_sa_family_t    nl_family;      /* AF_NETLINK   */
        unsigned short  nl_pad;         /* zero         */
        __u32           nl_pid;         /* port ID      */
        __u32           nl_groups;      /* multicast groups mask */
};

struct nlmsghdr {
        __u32           nlmsg_len;      /* Length of message including header */
        __u16           nlmsg_type;     /* Message content */
        __u16           nlmsg_flags;    /* Additional flags */
        __u32           nlmsg_seq;      /* Sequence number */
        __u32           nlmsg_pid;      /* Sending process port ID */
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;两个重要的结构, 不过注释很清楚了,其中nl_groups, 作为bind 函数的参数，用于把调用进程加入到该nl_groups指定的多播组，如果设置为 0，表示调用者不加入任何多播组。作为sendto等函数的参数时。若nl_groups为0，配合nl_pid发送单播数据，当nl_groups不为0，配合nl_pid发送多播。
另外，　nl_pid, to kernel, =0, to user multicast , =0.&lt;/p&gt;

&lt;h4 id=&quot;tlvtype-length-value&quot;&gt;TLV(Type-Length-Value)&lt;/h4&gt;

&lt;p&gt;这个格式应该不会陌生，　使用的就是:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct nlattr {
        __u16           nla_len;
        __u16           nla_type;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正是netlink message 的消息格式。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;创建&lt;/h3&gt;

&lt;h4 id=&quot;section-2&quot;&gt;内核态&lt;/h4&gt;

&lt;p&gt;netlink_kernel_create(), net/netlink/af_netlink.c&lt;/p&gt;

&lt;p&gt;在这个函数中：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;先创建struct socket, 通过socket_create_lite()&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;再创建struct sock, 通过__netlink_create(), 并与上面的socket进行关联, 当然netlink_ops与netlink_proto都是必传的属性， 不过与用户态创建不同的是进入路径不再是通过的-&amp;gt;create().&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;下来还有很关键一步， nlk_sk(sk)-&amp;gt;netlink_rcv = cfg-&amp;gt;input， 绑定它的接收处理函数.而cfg就是创建时由各自模块传入的一个参数，如rtnetlink中&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; struct netlink_kernel_cfg cfg = {
         .groups         = RTNLGRP_MAX,
         .input          = rtnetlink_rcv,
         .cb_mutex       = &amp;amp;rtnl_mutex,
         .flags          = NL_CFG_F_NONROOT_RECV,
 };
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当然还要将它添加到table-&amp;gt;hash中呢，netlink_insert(sk, net, 0)，注意第三个参数一定是0,内核中的port id 总是为0.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;最后就是对应的nl_table表项中相应的实例进行添加.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;section-3&quot;&gt;用户态&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;通过 PF_NETLINK协议族，最终会调用netlink_create.下面是一个协议族该有的属性：

    static const struct net_proto_family netlink_family_ops = {
            .family = PF_NETLINK,
            .create = netlink_create,
            .owner  = THIS_MODULE,  /* for consistency 8) */
    };

    static struct proto netlink_proto = {
            .name     = &quot;NETLINK&quot;,
            .owner    = THIS_MODULE,
            .obj_size = sizeof(struct netlink_sock),
    };

    static const struct proto_ops netlink_ops = {
            .family =       PF_NETLINK,
            .owner =        THIS_MODULE,
            .bind =         netlink_bind,
            .connect =      netlink_connect,
            .sendmsg =      netlink_sendmsg,
            .recvmsg =      netlink_recvmsg,
            ....
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;与内核态相同的是都会调用__netlink_create()进行struct sock的创建，　而struct socket的创建由于走的是通用socket接口因些此时已经被创建.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;由于是内核态创建套接字在先，　因些可以获取到相应的nl_table[protocol], 并拿到对应的一些属性，如bind, 赋值给nlk-&amp;gt;netlink_bind.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;bind&quot;&gt;bind&lt;/h3&gt;

&lt;p&gt;通用socket接口，最终会调用netlink_bind()完成bind.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;如果指定了port id，就使用netlink_insert(), 这个函数中关键性的两步是:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; head = nl_portid_hashfn(hash, portid);
 ....
 nlk_sk(sk)-&amp;gt;portid = portid;
 sk_add_node(sk, head);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;下来就是对group的处理了.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;connect&quot;&gt;connect&lt;/h3&gt;

&lt;p&gt;通用socket接口，netlink_connect().&lt;/p&gt;

&lt;p&gt;没做很多工作，就是设置了下目的地址。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    if (err == 0) {
            sk-&amp;gt;sk_state    = NETLINK_CONNECTED;
            nlk-&amp;gt;dst_portid = nladdr-&amp;gt;nl_pid;
            nlk-&amp;gt;dst_group  = ffs(nladdr-&amp;gt;nl_groups);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-4&quot;&gt;发送&lt;/h3&gt;

&lt;h4 id=&quot;section-5&quot;&gt;用户态&lt;/h4&gt;

&lt;p&gt;netlink_sendmsg()&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;先对组播进行处理，netlink_broadcast(), 组播的路径为:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; netlink_broadcast_filtered-&amp;gt;do_one_broadcast-&amp;gt;netlink_broadcast_deliver-&amp;gt;(netlink_skb_set_owner_r)__netlink_sendskb

 sk_for_each_bound(sk, &amp;amp;nl_table[ssk-&amp;gt;sk_protocol].mc_list)
             do_one_broadcast(sk, &amp;amp;info);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;现在也该清楚mc_list里面是放什么的了, 在bind的时候会将有组播号的sock加入进来。方便在组播时快速找到，这里只是一个粗略的查找，对于精确的对于是在do_one_broadcast()中&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; if (nlk-&amp;gt;portid == p-&amp;gt;portid || p-&amp;gt;group - 1 &amp;gt;= nlk-&amp;gt;ngroups ||
     !test_bit(p-&amp;gt;group - 1, nlk-&amp;gt;groups))
         goto out;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;netlink_unicast()中再做处理, 其中代码:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; sk = netlink_getsockbyportid(ssk, portid);
 ...
 if (netlink_is_kernel(sk))
        return netlink_unicast_kernel(sk, skb, ssk);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;由于内核态的发送也是调用netlink_unicast()在这里就要做一下区分，如果是发送给kernel会走另一条路径。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在netlink_getsockbyportid()中会通过netlink_lookup()在相应的nl_table-&amp;gt;hash中查找。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; head = nl_portid_hashfn(hash, portid);
 sk_for_each(sk, head) {
         if (table-&amp;gt;compare(net, sk) &amp;amp;&amp;amp;
             (nlk_sk(sk)-&amp;gt;portid == portid)) {
                 sock_hold(sk);
                 goto found;
         }
 }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;然后调用netlink_unicast_kernel():&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; if (nlk-&amp;gt;netlink_rcv != NULL) {
         ret = skb-&amp;gt;len;
         netlink_skb_set_owner_r(skb, sk);
         NETLINK_CB(skb).sk = ssk;
         netlink_deliver_tap_kernel(sk, ssk, skb);
         nlk-&amp;gt;netlink_rcv(skb);
         consume_skb(skb);
 } else {
         kfree_skb(skb);
 }
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;在这个函数当中，　会将skb bound到sk上，再调用对应创建时指定的netlink_rcv函数。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;section-6&quot;&gt;内核态&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;对于组播的，　使用netlink_broadcast(),等同于用户态的&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对于单播的，　使用netlink_unicast(), 在用户态的第2步时，　会选择执行netlink_attachskb(),将套接字绑定，继续往下走&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;netlink_sendskb()中会调用skb_queue_tail(&amp;amp;sk-&amp;gt;sk_receive_queue, skb);将skb 挂在sock的接收队列上,&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-7&quot;&gt;接收&lt;/h3&gt;

&lt;p&gt;内核态的接收是被注册后，　被动触发的&lt;/p&gt;

&lt;p&gt;用户态觉得也没什么可说的，　重要的就是从缓冲区中拿数据.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>linux 相关的一些书</title>
   <link href="http://lxin.org/linux%20kernel/2014/04/01/some-books-about-linux-kernel"/>
   <updated>2014-04-01T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20kernel/2014/04/01/some-books-about-linux-kernel</id>
   <content type="html">
&lt;h4 id=&quot;linux--&quot;&gt;1.Linux内核完全剖析–赵炯&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;一本关于很好内核入门的书，很细致了讲了linux 0.11的源码。
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;linux---robertlove&quot;&gt;2.Linux内核设计与实现 –RobertLove&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;内核的主要架构,更多是原理性的
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;understanding-the-linux-kernel--bovet--cesati&quot;&gt;3.Understanding the Linux Kernel–Bovet &amp;amp; Cesati&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;更深入的讲解了内核的框架，讲了一些源码，不过没有提到网络相关的东西,那会觉得 fs/ipc/task讲得很好.
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;linux-26---&quot;&gt;4.linux 2.6内核标准教程 –河秦，王洪涛&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;第一次看到的国内linux的书，觉得memery与interrupt讲得比较好
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;linux---&quot;&gt;5.linux设备驱动开发详解 –宋宝华&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;不是讲driver最经典的书，但个人觉得还有一些用，经典当手册来查
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;linux---1&quot;&gt;6.linux网络内核分析与开发–消宇峰&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;一本现在看来不怎么样的书，但在一开始通过它看到了整个协议栈的框架, 其实ppp协议讲得还是挺不错的
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;understanding-linux-network-internals---christian-benvenuti&quot;&gt;7.Understanding Linux Network Internals –Christian Benvenuti&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;内核网络经典，ipv4/bridge/neigh/route讲得很好
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;linuxtcpip---&quot;&gt;8.linux内核源码剖析－tcp/ip实现 –樊东东,莫澜&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;讲得很多，但个人觉得tcp协议的实现讲得最好
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;linux-kernel-networking-implementation-and-theory---rami-rosen&quot;&gt;9 Linux Kernel Networking Implementation and Theory –Rami Rosen&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;挺新的一本书，好的是上面大致的讲了下ipv6/ipsec.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上是从大二开始接触kernel以来看过的一些觉得好的书，　不好的就不提了，　是按时间顺序写的，现在想想也是linux学习挺合理的顺序&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>写的几个有用的module</title>
   <link href="http://lxin.org/linux%20tool/2014/03/17/some-useful-debug-module"/>
   <updated>2014-03-17T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20tool/2014/03/17/some-useful-debug-module</id>
   <content type="html">
&lt;h3 id=&quot;section&quot;&gt;1.内核态绕过协议栈发包&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;linux/module.h&amp;gt;
#include &amp;lt;linux/kernel.h&amp;gt;
#include &amp;lt;linux/init.h&amp;gt;
#include &amp;lt;linux/netfilter.h&amp;gt;
#include &amp;lt;linux/skbuff.h&amp;gt;
#include &amp;lt;linux/ip.h&amp;gt;
#include &amp;lt;linux/inet.h&amp;gt;
#include &amp;lt;linux/netdevice.h&amp;gt;
#include &amp;lt;linux/if_ether.h&amp;gt;
#include &amp;lt;linux/if_packet.h&amp;gt;
#include &amp;lt;net/udp.h&amp;gt;
#include &amp;lt;net/udp.h&amp;gt;
#include &amp;lt;net/route.h&amp;gt;
#include &amp;lt;net/icmp.h&amp;gt;
#include &amp;lt;linux/netfilter_ipv4.h&amp;gt;

MODULE_LICENSE(&quot;GPL&quot;);
MODULE_AUTHOR(&quot;LUCIEN&quot;);

#define ETH &quot;eth4&quot;
#define SIP &quot;192.168.1.40&quot;
#define DIP &quot;192.168.1.19&quot;
#define SPORT 8000
#define DPORT 8000

unsigned char SMAC[ETH_ALEN] = {0x00,0x00,0xC9,0xE6,0x28,0xE8};
//unsigned char DMAC[ETH_ALEN] = {0x90,0xE2,0xBA,0x4A,0x64,0xC0};
unsigned char DMAC[ETH_ALEN] = {0x00,0x1B,0x21,0xA0,0x94,0xB6};


unsigned char pkt[8977]=&quot;123&quot;;

int cp_dev_xmit_udp (char * eth, u_char * smac, u_char * dmac,
				u_char * pkt, int pkt_len,
				u_long sip, u_long dip,
				u_short sport, u_short dport, u_long seq, u_long ack_seq, u_char psh, u_char fin)
{
		struct sk_buff * skb = NULL;
		struct net_device * dev = NULL;
		struct ethhdr * ethdr = NULL;
		struct iphdr * iph = NULL;
		struct udphdr * udph = NULL;
		u_char * pdata = NULL;
		int nret = 1;
		
		if (NULL == smac || NULL == dmac) 
			goto out;
		
		dev = dev_get_by_name(&amp;amp;init_net, eth);
		if (NULL == dev)
			goto out;
		skb = alloc_skb (pkt_len + sizeof (struct iphdr) + sizeof (struct udphdr) + LL_RESERVED_SPACE (dev), GFP_ATOMIC);

		if (NULL == skb)
				goto out;
		skb_reserve (skb, LL_RESERVED_SPACE (dev));//add data and tail
		skb-&amp;gt;dev = dev;
		skb-&amp;gt;pkt_type = PACKET_OTHERHOST;
		skb-&amp;gt;protocol = __constant_htons(ETH_P_IP);
		skb-&amp;gt;ip_summed = CHECKSUM_NONE;
		skb-&amp;gt;priority = 0;
		
		skb_set_network_header(skb, 0); //skb-&amp;gt;network_header = skb-&amp;gt;data + 0;
		skb_put(skb, sizeof (struct iphdr)); //add tail and len
		
		skb_set_transport_header(skb, sizeof (struct iphdr));//skb-&amp;gt;transport_header = skb-&amp;gt;data + sizeof (struct iphdr)
		skb_put(skb, sizeof (struct udphdr));
		
		pdata = skb_put (skb, pkt_len);
		{
				if (NULL != pkt)
						memcpy (pdata, pkt, pkt_len);
		}

		{
				udph = udp_hdr(skb);
				memset (udph, 0, sizeof (struct udphdr));
				udph-&amp;gt;source = sport;
				udph-&amp;gt;dest = dport;
				udph-&amp;gt;len = htons(pkt_len+sizeof(struct udphdr)); //remember htons
				udph-&amp;gt;check = 0;
		}

		{
				iph = ip_hdr(skb);
				iph-&amp;gt;version = 4;
				iph-&amp;gt;ihl = sizeof(struct iphdr)&amp;gt;&amp;gt;2;
				iph-&amp;gt;frag_off = 0;
				iph-&amp;gt;protocol = IPPROTO_UDP;
				iph-&amp;gt;tos = 0;
				iph-&amp;gt;daddr = dip;
				iph-&amp;gt;saddr = sip;
				iph-&amp;gt;ttl = 0x40;
				iph-&amp;gt;tot_len = __constant_htons(skb-&amp;gt;len);
				iph-&amp;gt;check = 0;//remember to set 0
				iph-&amp;gt;check = ip_fast_csum((void *)iph,iph-&amp;gt;ihl);
		}

		{
				ethdr = (struct ethhdr*)skb_push (skb, 14);//reduce data and add len
				memcpy (ethdr-&amp;gt;h_dest, dmac, ETH_ALEN);
				memcpy (ethdr-&amp;gt;h_source, smac, ETH_ALEN);
				ethdr-&amp;gt;h_proto = __constant_htons (ETH_P_IP);
		}
				printk(&quot;%d\n&quot;, udph-&amp;gt;len);
		if (0 &amp;gt; dev_queue_xmit(skb)) goto out;
		nret = 0;
out:
		if (0 != nret &amp;amp;&amp;amp; NULL != skb)
		{
				dev_put (dev);
				kfree_skb (skb);
		}
		return (nret);
}

static int __init init(void)
{
		cp_dev_xmit_udp (ETH, SMAC, DMAC,pkt, sizeof(pkt),
						in_aton(SIP),in_aton(DIP),
						htons(SPORT),htons(DPORT),
						1, 0, 0, 0);
		return 0;
}

static void __exit fini(void)
{
}

module_init(init);
module_exit(fini);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;kprobe-&quot;&gt;2.kprobe 内核调试通用模板&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;linux/module.h&amp;gt;
#include &amp;lt;linux/kernel.h&amp;gt;
#include &amp;lt;linux/string.h&amp;gt;
#include &amp;lt;linux/init.h&amp;gt;
#include &amp;lt;linux/kprobes.h&amp;gt;
#include &amp;lt;linux/kallsyms.h&amp;gt;
#include &quot;asm/ptrace.h&quot; 
#include &quot;asm/current.h&quot; 
#include &quot;linux/utsname.h&quot; 
#include &quot;linux/tcp.h&quot;      
#include &quot;linux/in.h&quot;

struct kprobe probe; 

static int pre_probe(struct kprobe *probe, struct pt_regs *regs) 
{ 
	printk(&quot;hit it\n&quot;);
	return 0; 
} 

static void post_probe(struct kprobe *probe, struct pt_regs *regs, unsigned long flags) 
{} 

static int __init kprobe_init(void) 
{ 
	probe.pre_handler = pre_probe; 
	probe.post_handler = post_probe; 

	probe.addr = (kprobe_opcode_t *) kallsyms_lookup_name(&quot;#####&quot;); 
	if (probe.addr == NULL) { 
		return 1; 
	} 

	register_kprobe(&amp;amp;probe); 
	printk(&quot;register probe driver.n&quot;); 
	return 0; 
} 

static void __exit kprobe_exit(void) 
{ 
	unregister_kprobe(&amp;amp;probe); 
	printk(&quot;unregister probe driver.n&quot;); 
	return; 
} 

module_init(kprobe_init); 
module_exit(kprobe_exit); 

MODULE_AUTHOR(&quot;LUCIEN&quot;); 
MODULE_LICENSE(&quot;GPL&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;netfilter-&quot;&gt;3.netfilter 调用&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;linux/module.h&amp;gt;
#include &amp;lt;linux/kernel.h&amp;gt;
#include &amp;lt;linux/ip.h&amp;gt;
#include &amp;lt;linux/netfilter_ipv4.h&amp;gt;
#include &amp;lt;net/protocol.h&amp;gt;

static struct nf_hook_ops nfho;

unsigned int hook_func(unsigned int hooknum,
                struct sk_buff **skb,
                const struct net_device *in,
                const struct net_device *out,
                int (*okfn)(struct sk_buff *))
{
        int protocol = (*skb)-&amp;gt;nh.iph-&amp;gt;protocol;
        if(protocol==132){
                struct sk_buff *skb_l = skb_clone(*skb,GFP_ATOMIC);
                okfn(skb_l);
                struct sk_buff *skb_l2 = skb_clone(*skb,GFP_ATOMIC);
                okfn(skb_l2);
                struct sk_buff *skb_l3 = skb_clone(*skb,GFP_ATOMIC);
                okfn(skb_l3);
        }
        return NF_ACCEPT;
}

int init_module()
{
        nfho.hook = hook_func;
        nfho.hooknum  = NF_IP_LOCAL_IN;
        nfho.pf       = PF_INET;
        nfho.priority = NF_IP_PRI_FIRST;

        nf_register_hook(&amp;amp;nfho);

        return 0;
}
void cleanup_module()
{
        nf_unregister_hook(&amp;amp;nfho);

}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>一些常用的自动化调试的命令</title>
   <link href="http://lxin.org/linux%20tool/2014/03/17/some-auto-debug-command"/>
   <updated>2014-03-17T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20tool/2014/03/17/some-auto-debug-command</id>
   <content type="html">
&lt;p&gt;1.快速创建kvm虚拟机&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;beaker_eth=`ip addr show |grep 10.66 |awk &#39;{print $NF}&#39;`
mac_addr=&quot;&quot;
gip_addr=&quot;&quot;

sys_cls()
{
	killall qemu-kvm &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 ;sleep 1
	killall dhclient &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 &amp;amp;&amp;amp; sleep 1
	dhclient br1  &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
	rm -f ips &amp;gt;/dev/null
}
sys_init()
{
    test -f /etc/yum.repos.d/beaker-tasks.repo &amp;amp;&amp;amp; rm -f /etc/yum.repos.d/beaker-tasks.repo
    rpm -qa |grep qemu-kvm &amp;gt;/dev/null || yum -y install qemu-kvm &amp;gt;/dev/null
    rpm -qa |grep tcpdump &amp;gt;/dev/null || yum -y install tcpdump &amp;gt;/dev/null
    rpm -qa |grep net-tools &amp;gt;/dev/null || yum -y install net-tools &amp;gt;/dev/null
    rpm -qa |grep bridge-utils &amp;gt;/dev/null || yum -y install bridge-utils &amp;gt;/dev/null
    rpm -qa |grep psmisc &amp;gt;/dev/null || yum -y install psmisc &amp;gt;/dev/null
    test -f /root/$img_name || wget http://10.66.13.12/sys/$img_name -O /root/$img_name
    return 0
}
get_mac()
{
	rstr=`date +%s`
	mac_addr=00:${rstr:0:2}:${rstr:2:2}:${rstr:4:2}:${rstr:6:2}:${rstr:8:2}
	return 0
}
new_swit()
{
	if [ ! -f /etc/qemu-ifup ];then
                cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/qemu-ifup 
#!/bin/sh
switch=br1
/sbin/ifconfig \$1 0.0.0.0 up
/usr/sbin/brctl addif \${switch} \$1
EOF
                chmod 755 /etc/qemu-ifup
        fi
	if [ ! -f /etc/qemu-ifdown ];then
                cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/qemu-ifdown
#!/bin/sh
switch=br1
/usr/sbin/brctl delif \${switch} \$1
EOF
                chmod 755 /etc/qemu-ifdown
	fi
	brctl show |grep br1 &amp;gt; /dev/null &amp;amp;&amp;amp; return 0
	brctl addbr br1 &amp;amp;&amp;amp; brctl addif br1 $beaker_eth || return -1
	killall dhclient &amp;gt; /dev/null; sleep 1;dhclient br1 || return -1
	ip addr flush $beaker_eth
	return 0
}
new_br()
{
	if [ ! -f /etc/qemu-ifup-br0 ];then
                cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/qemu-ifup-br0
#!/bin/sh
switch=br0
/sbin/ifconfig \$1 0.0.0.0 up
/usr/sbin/brctl addif \${switch} \$1
EOF
                chmod 755 /etc/qemu-ifup-br0
        fi
	if [ ! -f /etc/qemu-ifdown-br0 ];then
                cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/qemu-ifdown-br0
#!/bin/sh
switch=br0
/usr/sbin/brctl delif \${switch} \$1
EOF
                chmod 755 /etc/qemu-ifdown-br0
        fi
	brctl show |grep ^br0 &amp;gt; /dev/null &amp;amp;&amp;amp; return 0
	brctl addbr br0 &amp;amp;&amp;amp; ip addr add 192.168.0.254/24 dev br0 || return -1
	ip link set br0 up
	return 0
}
start_vm()
{
	/usr/libexec/qemu-kvm -name vm1 \
	-drive file=/root/$img_name,if=none,id=drive-virtio-disk1,media=disk,cache=none,snapshot=off,format=qcow2,aio=native  \
	-device virtio-blk-pci,drive=drive-virtio-disk1,id=virtio-disk1,bootindex=0  \
	-netdev tap,id=hostnet0,vhost=on,script=/etc/qemu-ifup,downscript=downscript=/etc/qemu-ifdown \
	-device virtio-net-pci,netdev=hostnet0,id=virtio-net-pci0,mac=$mac_addr \
	$kvm_append \
	-smp 2,cores=1,threads=1,sockets=2  \
	-m 4096 &amp;gt;&amp;gt;./ips 2&amp;gt;&amp;amp;1 &amp;amp;
	return 0
}
get_gip()
{
	gip_addr=`timeout 200 tcpdump -i br1 ether src $mac_addr and arp[7]=1 and arp[15]!=0 -n -c 1 2&amp;gt;/dev/null| awk -F&#39; |,&#39; &#39;{print $8}&#39;`
	test -z $gip_addr &amp;amp;&amp;amp; return -1
	sleep 1;
	return 0;
}
exp_run()
{
	./sshrun $gip_addr $!
}
start_kvm()
{
	sys_init || { echo &quot;sys init fail&quot; ; exit -1;} &amp;amp;&amp;amp; echo &quot;sys init&quot;
	get_mac  || { echo &quot;get mac  fail&quot; ; exit -2;} &amp;amp;&amp;amp; echo &quot;get mac &quot;$mac_addr
	new_swit || { echo &quot;new swit fail&quot; ; exit -3;} &amp;amp;&amp;amp; echo &quot;net public br&quot;
	new_br   || { echo &quot;new br0  fail&quot; ; exit -4;} &amp;amp;&amp;amp; echo &quot;net pravite br&quot;
	start_vm || { echo &quot;start vm fail&quot; ; exit -5;} &amp;amp;&amp;amp; echo &quot;start vm &quot;
	get_gip  || { echo &quot;get gip  fail&quot; ; exit -6;} &amp;amp;&amp;amp; echo $gip_addr &amp;gt;&amp;gt; ips
	echo &quot;successfully&quot;
}

##### main
sys_cls
#mac_addr=&quot;00:13:93:40:04:78&quot;
img_name=&quot;rhel7.qcow2&quot;
kvm_append=&quot;-netdev tap,id=hostnet1,vhost=on,script=/etc/qemu-ifup-br0,downscript=/etc/qemu-ifdown-br0 \
-device virtio-net-pci,netdev=hostnet1,id=virtio-net-pci1,mac=00:00:00:00:00:01 \
-netdev tap,id=hostnet2,vhost=on,script=/etc/qemu-ifup-br0,downscript=/etc/qemu-ifdown-br0 \
-device virtio-net-pci,netdev=hostnet2,id=virtio-net-pci2,mac=00:00:00:00:00:02 \
-serial pty -vnc :10&quot;
start_kvm
#mac_addr=&quot;00:13:93:40:04:37&quot;
img_name=&quot;rhel7.1.qcow2&quot;
kvm_append=&quot;-netdev tap,id=hostnet1,vhost=on,script=/etc/qemu-ifup-br0,downscript=/etc/qemu-ifdown-br0 \
-device virtio-net-pci,netdev=hostnet1,id=virtio-net-pci1,mac=00:00:00:00:00:03 \
-netdev tap,id=hostnet2,vhost=on,script=/etc/qemu-ifup-br0,downscript=/etc/qemu-ifdown-br0 \
-device virtio-net-pci,netdev=hostnet2,id=virtio-net-pci2,mac=00:00:00:00:00:04 \
-serial pty -vnc :20&quot;
start_kvm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.sriov测试自动化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;beaker_eth=`ip addr show |grep 10.66 |awk &#39;{print $NF}&#39;`
mac_addr=&quot;&quot;
gip_addr=&quot;&quot;
sriov_dev=&quot;eth4&quot;
sriov_bus_brief=&quot;&quot;

sys_cls()
{
	killall qemu-kvm &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 ;sleep 1
	rm -f ips &amp;gt;/dev/null
}
sys_init()
{
	test -f /etc/yum.repos.d/beaker-tasks.repo &amp;amp;&amp;amp; rm -f /etc/yum.repos.d/beaker-tasks.repo
        rpm -qa |grep qemu-kvm &amp;gt;/dev/null || yum -y install qemu-kvm &amp;gt;/dev/null
        test -f /root/$img_name || wget http://10.66.13.12/sys/$img_name -O /root/$img_name
	return 0
}
get_mac()
{
	rstr=`date +%s`
	mac_addr=00:${rstr:0:2}:${rstr:2:2}:${rstr:4:2}:${rstr:6:2}:${rstr:8:2}
	return 0
}
new_swit()
{
	if [ ! -f /etc/qemu-ifup ];then
                cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/qemu-ifup 
#!/bin/sh
switch=switch
/sbin/ifconfig \$1 0.0.0.0 up
/usr/sbin/brctl addif \${switch} \$1
EOF
                chmod 755 /etc/qemu-ifup
        fi
	if [ ! -f /etc/qemu-ifdown ];then
                cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/qemu-ifdown
#!/bin/sh
switch=switch
/usr/sbin/brctl delif \${switch} \$1
EOF
                chmod 755 /etc/qemu-ifdown
	fi
	brctl show |grep switch &amp;gt; /dev/null &amp;amp;&amp;amp; return 0
	brctl addbr switch &amp;amp;&amp;amp; brctl addif switch $beaker_eth || return -1
	killall dhclient &amp;gt; /dev/null; sleep 1;dhclient switch || return -1
	ip addr flush $beaker_eth
	return 0
}
new_br()
{
	if [ ! -f /etc/qemu-ifup-br0 ];then
                cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/qemu-ifup-br0
#!/bin/sh
switch=br0
/sbin/ifconfig \$1 0.0.0.0 up
/usr/sbin/brctl addif \${switch} \$1
EOF
                chmod 755 /etc/qemu-ifup-br0
        fi
	if [ ! -f /etc/qemu-ifdown-br0 ];then
                cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/qemu-ifdown-br0
#!/bin/sh
switch=br0
/usr/sbin/brctl delif \${switch} \$1
EOF
                chmod 755 /etc/qemu-ifdown-br0
        fi
	brctl show |grep br0 &amp;gt; /dev/null &amp;amp;&amp;amp; return 0
	brctl addbr br0 &amp;amp;&amp;amp; ip addr add 192.168.0.254/24 dev br0 || return -1
	ip link set br0 up
	return 0
}
start_vm()
{
	/usr/libexec/qemu-kvm -name vm1 \
	-drive file=/root/$img_name,if=none,id=drive-virtio-disk1,media=disk,cache=none,snapshot=off,format=qcow2,aio=native  \
	-device virtio-blk-pci,drive=drive-virtio-disk1,id=virtio-disk1,bootindex=0  \
	-netdev tap,id=hostnet0,vhost=on,script=/etc/qemu-ifup,downscript=downscript=/etc/qemu-ifdown \
	-device virtio-net-pci,netdev=hostnet0,id=virtio-net-pci0,mac=$mac_addr \
	$kvm_append \
	-smp 2,cores=1,threads=1,sockets=2  \
	-m 4096 &amp;gt;&amp;gt;./ips 2&amp;gt;&amp;amp;1 &amp;amp;
	return 0
}
get_gip()
{
	gip_addr=`timeout 200 tcpdump -i switch ether src $mac_addr and arp[7]=1 and arp[15]!=0 -n -c 1 2&amp;gt;/dev/null| awk -F&#39; |,&#39; &#39;{print $8}&#39;`
	test -z $gip_addr &amp;amp;&amp;amp; return -1
	sleep 1;
	return 0;
}
exp_run()
{
	./sshrun $gip_addr $!
}
start_kvm()
{
	sys_init || { echo &quot;sys init fail&quot; ; exit -1;} &amp;amp;&amp;amp; echo &quot;sys init&quot;
	get_mac  || { echo &quot;get mac  fail&quot; ; exit -2;} &amp;amp;&amp;amp; echo &quot;get mac &quot;$mac_addr
	new_swit || { echo &quot;new swit fail&quot; ; exit -3;} &amp;amp;&amp;amp; echo &quot;net public br&quot;
	new_br   || { echo &quot;new br0  fail&quot; ; exit -4;} &amp;amp;&amp;amp; echo &quot;net pravite br&quot;
	start_vm || { echo &quot;start vm fail&quot; ; exit -5;} &amp;amp;&amp;amp; echo &quot;start vm &quot;
	get_gip  || { echo &quot;get gip  fail&quot; ; exit -6;} &amp;amp;&amp;amp; echo $gip_addr &amp;gt;&amp;gt; ips
	echo &quot;successfully&quot;
}
enable_sriov()
{
	ip link set $sriov_dev up
	dev_bus=`ethtool -i $sriov_dev |grep bus-info |awk &#39;{print $2}&#39;`
	echo 1 &amp;gt; /sys/bus/pci/devices/$dev_bus/sriov_numvfs
	sriov_bus_brief=`lspci|grep &quot;Virtual Function&quot; |awk &#39;{print $1}&#39;`
	sriov_bus=0000:$sriov_bus_brief
	local sriov_num_tmp=`lspci -n -s $sriov_bus | awk &#39;{ print $3 }&#39;`
	sriov_num=${sriov_num_tmp/:/ }
	echo $sriov_num &amp;gt; /sys/bus/pci/drivers/pci-stub/new_id
	echo $sriov_bus &amp;gt; /sys/bus/pci/devices/$sriov_bus/driver/unbind
	echo $sriov_bus &amp;gt; /sys/bus/pci/drivers/pci-stub/bind
	echo 1 &amp;gt; /sys/module/kvm/parameters/allow_unsafe_assigned_interrupts
}
##### main
sys_cls
enable_sriov || { echo &quot;enable sriov fail&quot;; exit -1;} &amp;amp;&amp;amp; echo &quot;enable sriov done&quot;
img_name=&quot;win2012-64-virtio.qcow2&quot;
kvm_append=&quot;-device pci-assign,host=$sriov_bus_brief,id=vfnet1 -vnc :3 -usb -device usb-tablet&quot;
start_kvm
&lt;/code&gt;&lt;/pre&gt;

</content>
 </entry>
 
 <entry>
   <title>kvm完成的网络测试</title>
   <link href="http://lxin.org/linux%20tool/2014/03/17/kvm-test"/>
   <updated>2014-03-17T00:00:00+08:00</updated>
   <id>hhttp://lxin.org/linux%20tool/2014/03/17/kvm-test</id>
   <content type="html">
&lt;p&gt;1.启动iso&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/libexec/qemu-kvm -name foo -m 1024 -hda foo.img -cdrom OneConnect-Flash-4.2.324.30.iso -boot d -vnc :1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.启动windows&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/libexec/qemu-kvm -name vm1 -nodefaults -chardev socket,id=hmp_id_humanmonitor1,path=/tmp/monitor-humanmonitor1-20130120-101609-Eyt4qYYS,server,nowait -mon chardev=hmp_id_humanmonitor1,mode=readline -chardev socket,id=serial_id_serial1,path=/tmp/serial-serial1-20130120-101609-Eyt4qYYS,server,nowait -device isa-serial,chardev=serial_id_serial1 -chardev socket,id=seabioslog_id_20130120-101609-Eyt4qYYS,path=/tmp/seabios-20130120-101609-Eyt4qYYS,server,nowait -drive file=/root/Tmp/winXP-32-virtio.qcow2,if=none,id=drive-virtio-disk1,media=disk,cache=none,boot=off,snapshot=off,format=qcow2,aio=native -device virtio-blk-pci,bus=pci.0,addr=0x5,drive=drive-virtio-disk1,id=virtio-disk1 -netdev tap,id=hostnet0,vhost=on,script=/etc/qemu-ifup -device virtio-net-pci,netdev=hostnet0,id=virtio-net-pci0,mac=00:60:4B:74:DE:10,bus=pci.0,addr=0x8  -m 4096 -smp 2,cores=1,threads=1,sockets=2  -M rhel6.4.0 -vnc :1 -vga cirrus -rtc base=utc,clock=host,driftfix=slew -boot order=cdn,once=c,menu=off -no-kvm-pit-reinjection -device sga -enable-kvm -usb -device usb-tablet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.macvtap网络模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip link add link eth11 name macvtap0 type macvtap
ip link set macvtap0 up
ip link show macvtap0

/usr/libexec/qemu-kvm -name vm1 -nodefaults -chardev socket,id=hmp_id_humanmonitor1,path=/tmp/monitor-humanmonitor1-20130120-101609-Eyt4qYYS,server,nowait -mon chardev=hmp_id_humanmonitor1,mode=readline -chardev socket,id=serial_id_serial1,path=/tmp/serial-serial1-20130120-101609-Eyt4qYYS,server,nowait -device isa-serial,chardev=serial_id_serial1 -chardev socket,id=seabioslog_id_20130120-101609-Eyt4qYYS,path=/tmp/seabios-20130120-101609-Eyt4qYYS,server,nowait -drive file=/root/RHEL6.5-20130913.0-64bit.qcow2,if=none,id=drive-virtio-disk1,media=disk,cache=none,boot=off,snapshot=off,format=qcow2,aio=native -device virtio-blk-pci,bus=pci.0,addr=0x5,drive=drive-virtio-disk1,id=virtio-disk1 -device virtio-net-pci,netdev=idsM2VQ0,mac=96:d7:ff:c8:e9:d0,bus=pci.0,addr=0x3,id=idsBAtyj 10&amp;lt;&amp;gt;/dev/tap10 -netdev tap,id=idsM2VQ0,vhost=on,fd=10 -m 4096 -smp 2,cores=1,threads=1,sockets=2  -M rhel6.4.0 -vnc :1 -vga cirrus -rtc base=utc,clock=host,driftfix=slew -boot order=cdn,once=c,menu=off -no-kvm-pit-reinjection -device sga -enable-kvm -monitor tcp:0:4444,server,nowait
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.sriov网络模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo 2 &amp;gt; /sys/bus/pci/devices/0000\:20\:00.0/sriov_numvfs
lspci -n -s 0000:21:01.0 | awk &#39;{ print $3 }&#39;
echo &quot;14e4 16af&quot; &amp;gt; /sys/bus/pci/drivers/pci-stub/new_id
echo 0000:21:01.0 &amp;gt; /sys/bus/pci/devices/0000\:21\:01.0/driver/unbind
echo 0000:21:01.0 &amp;gt; /sys/bus/pci/drivers/pci-stub/bind
echo 1 &amp;gt; /sys/module/kvm/parameters/allow_unsafe_assigned_interrupts

/usr/libexec/qemu-kvm -name vm1 -nodefaults -chardev socket,id=hmp_id_humanmonitor1,path=/tmp/monitor-humanmonitor1-20130120-101609-Eyt4qYYS,server,nowait -mon chardev=hmp_id_humanmonitor1,mode=readline -chardev socket,id=serial_id_serial1,path=/tmp/serial-serial1-20130120-101609-Eyt4qYYS,server,nowait -device isa-serial,chardev=serial_id_serial1 -chardev socket,id=seabioslog_id_20130120-101609-Eyt4qYYS,path=/tmp/seabios-20130120-101609-Eyt4qYYS,server,nowait -drive file=/root/RHEL6.5-20130913.0-64bit.qcow2,if=none,id=drive-virtio-disk1,media=disk,cache=none,boot=off,snapshot=off,format=qcow2,aio=native -device virtio-blk-pci,bus=pci.0,addr=0x5,drive=drive-virtio-disk1,id=virtio-disk1 -device pci-assign,host=20:01.0,id=vfnet1 -m 4096 -smp 2,cores=1,threads=1,sockets=2 -M rhel6.4.0 -vnc :1 -vga cirrus -rtc base=utc,clock=host,driftfix=slew -boot order=cdn,once=c,menu=off -no-kvm-pit-reinjection -device sga -enable-kvm -monitor tcp:0:4444,server,nowait
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5.br网络模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
switch=br0
/sbin/ifconfig $1 0.0.0.0 up
/usr/sbin/brctl addif ${switch} $1


/usr/libexec/qemu-kvm -name vm1 -nodefaults -chardev socket,id=hmp_id_humanmonitor1,path=/tmp/monitor-humanmonitor1-20130120-101609-Eyt4qYYS,server,nowait -mon chardev=hmp_id_humanmonitor1,mode=readline -chardev socket,id=serial_id_serial1,path=/tmp/serial-serial1-20130120-101609-Eyt4qYYS,server,nowait -device isa-serial,chardev=serial_id_serial1 -chardev socket,id=seabioslog_id_20130120-101609-Eyt4qYYS,path=/tmp/seabios-20130120-101609-Eyt4qYYS,server,nowait -drive file=./RHEL6.5-20130913.0-64bit.qcow2,if=none,id=drive-virtio-disk1,media=disk,cache=none,boot=off,snapshot=off,format=qcow2,aio=native -device virtio-blk-pci,bus=pci.0,addr=0x5,drive=drive-virtio-disk1,id=virtio-disk1 -netdev tap,id=hostnet0,vhost=on,script=/etc/qemu-ifup -device virtio-net-pci,netdev=hostnet0,id=virtio-net-pci0,mac=00:22:15:27:54:6a,bus=pci.0,addr=0x8  -m 4096 -smp 2,cores=1,threads=1,sockets=2  -M rhel6.4.0 -vnc :3 -vga cirrus -rtc base=utc,clock=host,driftfix=slew -boot order=cdn,once=c,menu=off -no-kvm-pit-reinjection -device sga -enable-kvm -monitor tcp:0:4444,server,nowait
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 
</feed>